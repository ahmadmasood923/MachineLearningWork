{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6550da-d47f-4443-824c-cc423effc56d",
   "metadata": {},
   "source": [
    "## Ques 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef64f3e-6485-48da-8543-42a7139f020e",
   "metadata": {},
   "source": [
    "### Ans: A contingency matrix, also known as a confusion matrix, is a table used to evaluate the performance of a classification model. It is a matrix that shows the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN) for each class in the classification problem.\n",
    "\n",
    "### The rows of the matrix represent the true classes of the instances, and the columns represent the predicted classes. Each entry in the matrix shows the number of instances that belong to a particular true class and are predicted to belong to a particular predicted class. For example, the entry in the first row and second column shows the number of instances that belong to the first true class but are predicted to belong to the second predicted class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e20b6a-d5ed-493b-a604-8504b901dcef",
   "metadata": {},
   "source": [
    "## Ques 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8ccf5-107c-4b8d-8850-acebe1701927",
   "metadata": {},
   "source": [
    "### Ans: A pair confusion matrix is a variant of the confusion matrix that is used to evaluate the performance of binary classification models when the focus is on comparing the performance of the model for each class. Unlike a regular confusion matrix, which is a square matrix that summarizes the overall performance of a classification model across all classes, a pair confusion matrix is a rectangular matrix that compares the performance of the model for each pair of classes.\n",
    "### For example, in a binary classification problem where the classes are positive and negative, a pair confusion matrix would have two rows and two columns, with the rows representing the true classes and the columns representing the predicted classes. The entries in the matrix would show the number of instances that belong to each pair of true and predicted classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d1003e-9eb1-4da6-a10c-1a81c3521e7f",
   "metadata": {},
   "source": [
    "## Ques 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36300835-10fd-4d38-b62d-0b593b960a91",
   "metadata": {},
   "source": [
    "### Ans: In the context of natural language processing (NLP), an extrinsic measure is a performance evaluation metric that measures the effectiveness of a language model in the context of a specific downstream task, such as text classification, information retrieval, or machine translation. The goal of an extrinsic evaluation is to assess how well a language model can solve a specific problem and to compare the performance of different models on the same task.\n",
    "\n",
    "### Extrinsic evaluation is typically carried out by training the language model on a large corpus of text and then evaluating its performance on a set of labeled data for the specific downstream task. The evaluation metric used for the task will depend on the task itself, and may include accuracy, precision, recall, F1 score, or other task-specific metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59c510e-cc9a-452d-b041-25a55a84fe2a",
   "metadata": {},
   "source": [
    "## Ques 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c046b0e7-0fc9-4666-ba4d-f328d6e28ce4",
   "metadata": {},
   "source": [
    "### Ans: In the context of machine learning, an intrinsic measure is a performance evaluation metric that measures the effectiveness of a model in a standalone or isolated setting, without considering the downstream task for which the model might be used. In other words, intrinsic evaluation focuses on how well a model captures certain aspects of the training data, rather than how well it performs on a specific task.\n",
    "### Intrinsic measures are commonly used in model development and selection, as they allow for the comparison of different models based on their ability to capture the underlying patterns and structure of the data. Examples of intrinsic measures include accuracy, precision, recall, F1 score, mean squared error, and other metrics that are calculated directly from the model predictions and the actual labels or targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7423be-1ff9-44ab-8f60-5105a7a494be",
   "metadata": {},
   "source": [
    "## Ques 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3974503-f4b3-41a2-84d0-375008fe26e9",
   "metadata": {},
   "source": [
    "### Ans: A confusion matrix is a table that summarizes the performance of a machine learning classification model on a set of data for which the true values are known. It is a useful tool for evaluating the performance of a model, as it provides a detailed breakdown of the number of correct and incorrect predictions made by the model for each class in the dataset.\n",
    "### A confusion matrix is typically organized into four categories: true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). These categories are defined as follows:\n",
    "### True positives (TP): The number of correct predictions the model made for the positive class.\n",
    "### False positives (FP): The number of incorrect predictions the model made for the positive class.\n",
    "### True negatives (TN): The number of correct predictions the model made for the negative class.\n",
    "### False negatives (FN): The number of incorrect predictions the model made for the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f9082-79f2-48e5-a446-e13e3e2b4649",
   "metadata": {},
   "source": [
    "## Ques 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766cd022-60d0-48fc-8bfe-a518e62fbc5d",
   "metadata": {},
   "source": [
    "### Ans: Unsupervised learning algorithms do not have a predefined set of classes or labels, so their performance evaluation metrics are different from supervised learning algorithms. Intrinsic measures are commonly used to evaluate the performance of unsupervised learning algorithms, and they typically involve comparing the clustering results produced by the algorithm to some pre-defined criteria. Here are some common intrinsic measures used to evaluate unsupervised learning algorithms:\n",
    "### Silhouette Coefficient: This measures the compactness and separation of clusters. It ranges from -1 to +1, where a higher value indicates better clustering results.\n",
    "### Calinski-Harabasz Index: This measures the ratio of between-cluster variance to within-cluster variance. It ranges from 0 to infinity, where a higher value indicates better clustering results.\n",
    "### Davies-Bouldin Index: This measures the average similarity between each cluster and its most similar cluster, relative to the average dissimilarity between each cluster and its least similar cluster. It ranges from 0 to infinity, where a lower value indicates better clustering results.\n",
    "### Dunn Index: This measures the ratio of the minimum distance between clusters to the maximum diameter of clusters. It ranges from 0 to infinity, where a higher value indicates better clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f16b8e-ceed-400c-9158-317116464339",
   "metadata": {},
   "source": [
    "## Ques 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de39e7f-2cba-4f7a-9dea-7ee5def7953b",
   "metadata": {},
   "source": [
    "### Ans: While accuracy is a widely used evaluation metric for classification tasks, it has several limitations:\n",
    "### Imbalanced classes: When the classes in the dataset are imbalanced, accuracy can be misleading. For example, if one class is much larger than the others, a classifier that always predicts that class will have a high accuracy even though it is not performing well overall. This can be addressed by using other evaluation metrics such as precision, recall, F1 score, or the area under the receiver operating characteristic (ROC) curve, which take into account both true positives and false positives.\n",
    "### Cost-sensitive classification: In some applications, the cost of misclassification may be different for different classes. For example, in medical diagnosis, a false negative (a person who is actually sick but is classified as healthy) may be more costly than a false positive (a person who is healthy but is classified as sick). In such cases, accuracy may not be the best evaluation metric, and more sophisticated measures such as weighted accuracy or cost-sensitive classification algorithms can be used.\n",
    "### Multiclass classification: When there are more than two classes, accuracy may not provide a complete picture of the performance of the classifier. For example, a classifier that performs well on some classes but poorly on others may have a high overall accuracy but not be useful in practice. In such cases, metrics such as macro-averaged precision, recall, and F1 score can be used to evaluate the performance of the classifier on each class separately.\n",
    "### Uncertainty and confidence: Accuracy does not provide any information about the confidence or uncertainty of the classifier's predictions. In some applications, it may be important to know how confident the classifier is in its predictions, especially if the predictions are used to make decisions. In such cases, probabilistic classifiers such as logistic regression or decision trees can be used, and metrics such as log loss or Brier score can be used to evaluate the calibration and uncertainty of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b8339-00eb-46b7-a656-7086a8e5fef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
