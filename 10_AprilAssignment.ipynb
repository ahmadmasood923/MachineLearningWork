{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9798e282-e478-4616-8241-7bb60e501512",
   "metadata": {},
   "source": [
    "## Ques 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87e998-f9e5-40af-9fc0-69ba5a6a356d",
   "metadata": {},
   "source": [
    "### Ans: To find the probability that an employee is a smoker given that they use the health insurance plan, we need to use Bayes' theorem:\n",
    "### P(Smoker | Uses insurance plan) = P(Uses insurance plan | Smoker) * P(Smoker) / P(Uses insurance plan)\n",
    "### We know that 70% of employees use the company's health insurance plan, so:\n",
    "### P(Uses insurance plan) = 0.7\n",
    "### We also know that 40% of the employees who use the plan are smokers, so:\n",
    "### P(Uses insurance plan | Smoker) = 0.4\n",
    "### Finally, we need to determine the probability of being a smoker, regardless of whether or not the person uses the insurance plan:\n",
    "### P(Smoker) = ?\n",
    "### Unfortunately, we don't have enough information to determine this directly from the survey results. However, we can make a reasonable assumption based on some general statistics:\n",
    "### Let's say that 20% of all employees are smokers. This is a rough estimate, but it's a common percentage in many populations.\n",
    "### With this assumption, we can calculate:\n",
    "### P(Smoker) = 0.2\n",
    "### Now we can substitute these values into Bayes' theorem:\n",
    "### P(Smoker | Uses insurance plan) = 0.4 * 0.2 / 0.7\n",
    "### P(Smoker | Uses insurance plan) ≈ 0.1143\n",
    "### Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.1143 or about 11.43%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25736866-06d2-4bd5-aae3-50ebd4eafa77",
   "metadata": {},
   "source": [
    "## Ques 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0116ff-ab6d-4704-a06d-5f8594e41f3d",
   "metadata": {},
   "source": [
    "### Ans: Both Bernoulli Naive Bayes and Multinomial Naive Bayes are variants of the Naive Bayes algorithm used in machine learning for classification problems. The main difference between the two lies in the type of input data they are best suited for.\n",
    "### Bernoulli Naive Bayes is generally used when the features (input variables) are binary, meaning they can take on only two values such as 0 or 1. It is named after the Bernoulli distribution, which models the probability of a binary event occurring. In Bernoulli Naive Bayes, each feature is assumed to be a binary variable that indicates whether or not a particular word or feature occurs in the text. This approach is commonly used in text classification tasks where the presence or absence of a particular word in a document is used as a feature for classification.\n",
    "### On the other hand, Multinomial Naive Bayes is used when the features are count-based, such as word frequencies. It is named after the multinomial distribution, which models the probability of multiple outcomes occurring from a discrete set. In Multinomial Naive Bayes, the frequency of occurrence of each word or feature in a document is used as a feature for classification. This approach is commonly used in tasks such as spam filtering, where the frequency of certain words or phrases in an email is used as a feature to determine whether the email is spam or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b47dd3-d7fd-42cc-923c-e8b6a5b04933",
   "metadata": {},
   "source": [
    "## Ques 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7f000-c570-4ca0-ac1b-6636cd3cfcc3",
   "metadata": {},
   "source": [
    "### Ans: Bernoulli Naive Bayes is a classification algorithm that assumes that the input features are binary (i.e., have only two possible values: 0 or 1), and it is commonly used in text classification tasks where the presence or absence of a word is used as a feature for classification. In such cases, if a word is missing in a document, it is considered as not present, which is equivalent to having a value of 0.\n",
    "### If a feature is missing in some data points, Bernoulli Naive Bayes handles the missing values by treating them as not present, or 0. For example, suppose we have a dataset where each data point represents a document, and the features are binary variables that indicate the presence or absence of certain words in the document. If some documents do not contain one or more of the words, those features will have a missing value, and they will be treated as 0 in the model.\n",
    "### When training the Bernoulli Naive Bayes model, the probability of a word given a class is estimated using the frequency of the word in the training documents that belong to that class. If a word is missing in some training documents, it will not contribute to the frequency count for that class, and it will be treated as if it does not exist in those documents. This means that missing values will not affect the estimation of the probabilities for the other features, and the model can still be trained and used for classification.\n",
    "### In summary, Bernoulli Naive Bayes handles missing values by treating them as not present, or 0, and estimating the probabilities based on the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575b26df-24bc-45bb-b26b-863dcccc9620",
   "metadata": {},
   "source": [
    "## Ques 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f29d59-32bb-4cdb-9586-a140d8440e81",
   "metadata": {},
   "source": [
    "### Ans: Yes, Gaussian Naive Bayes can be used for multi-class classification problems. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that is used when the features (input variables) are continuous and follow a Gaussian distribution. In multi-class classification problems, there are multiple classes or categories that each data point can be assigned to.\n",
    "### To use Gaussian Naive Bayes for multi-class classification, the algorithm needs to be extended to handle multiple classes. One common approach is to use a \"one-vs-all\" strategy, where the model is trained to classify each class separately from all the others. This involves training a separate binary classification model for each class, where the target variable is 1 for data points in that class and 0 for all other classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b2064b-195e-4e25-9944-729173294308",
   "metadata": {},
   "source": [
    "## Ques 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776c00bb-00c1-4045-b139-36e3a6cd9ab1",
   "metadata": {},
   "source": [
    "### Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7139cefe-4ea1-4aa0-bae0-6fde5341a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db2f0ed6-7615-4eed-a700-cd7c2f63bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
    "data = pd.read_csv(url, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98c9035-6807-48be-923e-f33f97b35bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e10d57e-5f65-420a-aefd-f7525e3dd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e5a82e-4fc9-47c2-970e-bedb6bea45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaafd524-ebb2-490b-9298-89a4c37f4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d109aff-6353-4d61-92fc-329707fefa05",
   "metadata": {},
   "source": [
    "### Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "401b1df0-7f05-4661-adbb-91094de2a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1deeaffc-89d4-4e2a-bf80-4f7c014c75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "533f0344-6278-4c5f-99ab-d8f52abd6914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8839380364047911\n",
      "Precision:  0.8869617393737383\n",
      "Recall:  0.8152389047416673\n",
      "f1_score:  0.8481249015095276\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ' ,cross_val_score(bernoulli_nb, X, y, cv=10, scoring='accuracy').mean())\n",
    "print('Precision: ',cross_val_score(bernoulli_nb, X, y, cv=10, scoring='precision').mean())\n",
    "print('Recall: ' ,cross_val_score(bernoulli_nb, X, y, cv=10, scoring='recall').mean())\n",
    "print('f1_score: ', cross_val_score(bernoulli_nb, X, y, cv=10, scoring='f1').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0fddc6-447d-416e-b935-3e936cf726d3",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c33a1531-dd7a-4711-956e-7b8e6732142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f525c422-aebb-4278-8564-7efc8fdcb104",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba57ab68-6be5-4be0-ad97-039c6bf9b1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7863496180326323\n",
      "Precision:  0.7393175533565436\n",
      "Recall:  0.7214983911116508\n",
      "f1_score:  0.7282909724016348\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ' ,cross_val_score(multinomial_nb, X, y, cv=10, scoring='accuracy').mean())\n",
    "print('Precision: ',cross_val_score(multinomial_nb, X, y, cv=10, scoring='precision').mean())\n",
    "print('Recall: ' ,cross_val_score(multinomial_nb, X, y, cv=10, scoring='recall').mean())\n",
    "print('f1_score: ', cross_val_score(multinomial_nb, X, y, cv=10, scoring='f1').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45329a-04a9-40c1-a26b-2efa117bfbcb",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d279d0d0-dda8-423f-8940-00e610c53114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3d79416-b777-4c16-a8e6-ed4058620ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db86f843-3356-48fd-80a5-9c3fabe67713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8217730830896915\n",
      "Precision:  0.7103733928118492\n",
      "Recall:  0.9569516119239877\n",
      "f1_score:  0.8130660909542995\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ' ,cross_val_score(gaussian_nb, X, y, cv=10, scoring='accuracy').mean())\n",
    "print('Precision: ',cross_val_score(gaussian_nb, X, y, cv=10, scoring='precision').mean())\n",
    "print('Recall: ' ,cross_val_score(gaussian_nb, X, y, cv=10, scoring='recall').mean())\n",
    "print('f1_score: ', cross_val_score(gaussian_nb, X, y, cv=10, scoring='f1').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dbe0f7-2dd2-4393-b609-4e029bb174c7",
   "metadata": {},
   "source": [
    "Based on the results, we can see that Gaussian Naive Bayes performed the best, followed by Bernoulli Naive Bayes and Multinomial Naive Bayes. One reason why Gaussian Naive Bayes performed the best is that it assumes that the input features are continuous and normally distributed, which is a better assumption for some datasets. In the Spambase dataset, some features are continuous, which makes Gaussian Naive Bayes a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159a7d4-a0be-4851-a6fe-311f3eaf62d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
