{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a3a5ae6-7fa3-4723-9718-7841b60024b4",
   "metadata": {},
   "source": [
    "## Ques 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c5d15-0362-49c6-9230-6aa957afeded",
   "metadata": {},
   "source": [
    "### Ans: Simple Linear Regression is a statistical technique used to model the relationship between a dependent variable and single independent variable. The aim of simple linear regression is to find the best straight line that describes the relationship between the two variables. The equation of a simple linear regression model is given by:\n",
    "### y = β0 + β1x + ε\n",
    "### where y is the dependent variable, x is the independent variable, β0 and β1 are the intercept and slope coefficients, respectively, and ε is the error term.\n",
    "### For example, consider a dataset that contains information on the number of hours studied and the score achieved on an exam. We can use simple linear regression to model the relationship between the number of hours studied (independent variable) and the score achieved on the exam (dependent variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d180f-59f9-40d6-8db3-7311c2a54c34",
   "metadata": {},
   "source": [
    "## Ques 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78bb336-ae1f-49f4-8f9c-548d927b9574",
   "metadata": {},
   "source": [
    "### Ans: Linear regression makes several assumptions about the data that must hold in order for the results of the analysis to be valid and reliable. These assumptions include:\n",
    "### Linearity: The relationship between the dependent variable and the independent variable(s) is linear.\n",
    "### Independence: The observations are independent of each other.\n",
    "### Homoscedasticity: The variance of the errors is constant across all levels of the independent variable(s).\n",
    "### Normality: The errors are normally distributed with a mean of zero.\n",
    "### No multicollinearity: The independent variables are not highly correlated with each other.\n",
    "### No influential outliers: There are no extreme or influential data points that can disproportionately affect the results of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29760ba4-0cb2-48e9-841f-1c4c765fcc1e",
   "metadata": {},
   "source": [
    "## Ques 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b39bec6-44a9-46a6-a121-4a91af6a9dd4",
   "metadata": {},
   "source": [
    "### Ans: In a linear regression model, the slope represents the change in the dependent variable for every one-unit change in the independent variable, while the intercept represents the value of the dependent variable when the independent variable is equal to zero.\n",
    "### For example, let's say we want to predict the price of a house based on its size in square feet. We collect data on the size and price of 100 houses in a particular city and perform a linear regression analysis. The resulting regression equation is:\n",
    "### Price = 100,000 + 200 x Size\n",
    "### In this equation, the intercept is 100,000, which means that the predicted price of a house with zero square feet is $100,000. However, this interpretation is not meaningful because a house cannot have zero square feet.\n",
    "### The slope of the equation is 200, which means that for every one-unit increase in the size of the house (in square feet), the price of the house is predicted to increase by $200. This is a meaningful interpretation since the slope tells us how the dependent variable (price) changes with a change in the independent variable (size). Therefore, we can say that the slope of the regression equation indicates the rate at which the price of a house increases with an increase in its size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a9d320-a061-48b6-931c-d4b17c7e6766",
   "metadata": {},
   "source": [
    "## Ques 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f47180-1e54-4258-9f44-cfc02ce7345e",
   "metadata": {},
   "source": [
    "### Ans: Gradient descent is an iterative optimization algorithm used in machine learning to find the minimum of a cost function. The cost function represents the difference between the predicted and actual values of the output variable, and the goal of gradient descent is to minimize this difference by adjusting the model's parameters.\n",
    "### The algorithm starts by initializing the model's parameters with random values and computes the gradient of the cost function with respect to each parameter. The gradient is the direction of steepest ascent of the cost function, and the negative of the gradient gives the direction of steepest descent. The algorithm then updates the model's parameters in the direction of the negative gradient, taking a step size determined by a learning rate hyperparameter. This process is repeated iteratively until the algorithm converges to a minimum of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f74d53-3c9b-486e-bd66-138153b43ce8",
   "metadata": {},
   "source": [
    "## Ques 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a7e40-5d2c-408b-99e4-fe2aa9747830",
   "metadata": {},
   "source": [
    "### Ans: Multiple linear regression is a statistical technique used to model the relationship between two or more independent variables (also known as predictors or features) and a single dependent variable (also known as the response or target variable).\n",
    "### In contrast to simple linear regression, which models the relationship between one independent variable and one dependent variable, multiple linear regression can incorporate multiple independent variables to make more complex predictions.\n",
    "### The general form of the multiple linear regression equation is:\n",
    "### y = β0 + β1x1 + β2x2 + ... + βnxn + ε"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08db394-d7ca-4156-a22f-ce9b550478d9",
   "metadata": {},
   "source": [
    "## Ques 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a524dd3-a2cd-40e1-9d15-7a555d82a975",
   "metadata": {},
   "source": [
    "### Ans: Multicollinearity is a phenomenon in multiple linear regression where two or more independent variables are highly correlated with each other. This can cause issues in the regression analysis, including unreliable coefficient estimates and reduced statistical power.\n",
    "### When multicollinearity is present, it can be difficult to determine the unique contribution of each independent variable to the dependent variable, as they are too closely related. In addition, the standard errors of the coefficients may be inflated, making it difficult to determine which variables are statistically significant.\n",
    "### To detect multicollinearity, a common approach is to calculate the correlation matrix of the independent variables. If two or more variables have a correlation coefficient greater than 0.7 or 0.8, they may be considered highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee458fc-c100-4895-a44f-0040072cc567",
   "metadata": {},
   "source": [
    "## Ques 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321f464-b64b-4e03-83cd-059c5ff73216",
   "metadata": {},
   "source": [
    "### Ans: Polynomial regression is a type of regression analysis where the relationship between the independent variable and dependent variable is modeled as an nth degree polynomial function. In contrast to linear regression, where the relationship between the independent variable and dependent variable is modeled as a straight line, polynomial regression allows for more complex nonlinear relationships between the variables.\n",
    "### The general form of the polynomial regression equation is:\n",
    "### y = β0 + β1x + β2x^2 + β3x^3 + ... + βnx^n + ε\n",
    "### where y is the dependent variable, x is the independent variable, β0, β1, β2, ..., βn are the coefficients, ε is the error term, and n is the degree of the polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe8ab3-88fd-4070-8efb-b0f898e9abca",
   "metadata": {},
   "source": [
    "## Ques 8:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b1bb1-a33c-4fb2-ba56-0191027a8f45",
   "metadata": {},
   "source": [
    "### Ans: Advantages of polynomial regression compared to linear regression include:\n",
    "### Flexibility: Polynomial regression can capture more complex, nonlinear relationships between the independent and dependent variables that cannot be captured by linear regression.\n",
    "### Improved accuracy: In cases where the relationship between the variables is curved, polynomial regression can provide a better fit to the data than linear regression.\n",
    "### Disadvantages of polynomial regression compared to linear regression include:\n",
    "### Overfitting: Polynomial regression models with high degrees of the polynomial may be prone to overfitting, meaning they may fit the training data very closely but generalize poorly to new data.\n",
    "### Increased complexity: Polynomial regression models with high degrees of the polynomial can be difficult to interpret and may require more computation time than linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21056e1-8914-4a70-96ea-c7730745f33e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
