{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6550da-d47f-4443-824c-cc423effc56d",
   "metadata": {},
   "source": [
    "## Ques 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94992f-c506-450a-a697-25b3d04202b3",
   "metadata": {},
   "source": [
    "### Ans: Anomaly detection is the process of identifying data points that deviate significantly from the norm or expected behavior of a dataset. Its purpose is to identify and flag unusual or potentially malicious events or behavior that may indicate the presence of outliers, anomalies, or rare events that are different from the majority of data points in the dataset. Anomaly detection can be applied to various domains, such as fraud detection, intrusion detection, fault detection, medical diagnosis, and predictive maintenance, among others. The goal of anomaly detection is to help identify and investigate potential threats or issues in a timely and accurate manner, thus enabling proactive actions to be taken to prevent or mitigate their impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb4e008-372c-4660-a3b3-385d94c196b5",
   "metadata": {},
   "source": [
    "## Ques 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1d4a0-3062-41b5-9c69-373bc07ba10b",
   "metadata": {},
   "source": [
    "### Ans: There are several key challenges in anomaly detection, including:\n",
    "### Lack of labeled data: Anomaly detection often requires labeled data for training, which may be difficult or expensive to obtain in many cases.\n",
    "### Imbalanced datasets: Anomalies are often rare events in a dataset, making it challenging to balance the dataset for accurate training and evaluation.\n",
    "### Scalability: As datasets grow in size, the computational complexity of anomaly detection algorithms can become a significant challenge.\n",
    "### Data quality: Anomalies can arise from measurement errors or missing data, which can be difficult to detect and handle.\n",
    "### Concept drift: Over time, the underlying distribution of data may shift, leading to the need for continuous monitoring and adaptation of anomaly detection models.\n",
    "### Interpretability: Many anomaly detection algorithms are black-box models, making it difficult to understand the factors that contribute to an anomaly or to provide explanations for their detections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdde575-300c-4aa9-b008-5792141f840b",
   "metadata": {},
   "source": [
    "## Ques 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666d037-4796-4734-84f1-bf8d0aea410e",
   "metadata": {},
   "source": [
    "### Ans: Unsupervised anomaly detection and supervised anomaly detection differ in the way they approach the problem of identifying anomalies in data:\n",
    "### Training data: In supervised anomaly detection, labeled training data is used to train a model to identify anomalies based on known examples of anomalies and non-anomalies. In unsupervised anomaly detection, there is no labeled data, and the algorithm must identify anomalies based on the underlying patterns and structure of the data.\n",
    "### Detection approach: In supervised anomaly detection, the model is trained to classify new data points as either anomalies or non-anomalies based on what it has learned from the labeled training data. In unsupervised anomaly detection, the algorithm must detect anomalies by identifying data points that deviate significantly from the expected pattern or behavior of the majority of data points.\n",
    "### Complexity: Supervised anomaly detection can be more complex and computationally intensive, as it requires training a model on labeled data. Unsupervised anomaly detection can be simpler and more efficient, as it does not require labeled data, but it may be less accurate due to the absence of ground truth labels.\n",
    "### Domain expertise: Supervised anomaly detection may require more domain expertise to identify and label anomalies, while unsupervised anomaly detection can be more exploratory, allowing for the discovery of new and unexpected anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4932a4e6-75d4-4c65-a6a0-e8ffe83ae627",
   "metadata": {},
   "source": [
    "## Ques 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1fe3f8-683b-4131-a432-9ab7b11526e1",
   "metadata": {},
   "source": [
    "### Ans: \n",
    "### Statistical-based methods: These methods assume that normal data points follow a known statistical distribution and identify anomalies as data points that deviate significantly from this distribution. Examples include Gaussian mixture models, kernel density estimation, and statistical process control.\n",
    "### Distance-based methods: These methods identify anomalies as data points that are significantly far away from the majority of other data points. Examples include k-nearest neighbor (k-NN), local outlier factor (LOF), and distance-based clustering.\n",
    "### Density-based methods: These methods identify anomalies as data points in regions of low data density or where the data density is significantly different from the surrounding regions. Examples include DBSCAN (Density-Based Spatial Clustering of Applications with Noise) and OPTICS (Ordering Points To Identify the Clustering Structure).\n",
    "### Clustering-based methods: These methods identify anomalies as data points that do not belong to any cluster or belong to a small or sparse cluster. Examples include k-means clustering and spectral clustering.\n",
    "### Machine learning-based methods: These methods use machine learning algorithms to learn the normal patterns of data and identify anomalies as data points that deviate significantly from these patterns. Examples include isolation forest, one-class SVM (Support Vector Machine), and autoencoder-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33bcec2-a2ff-4794-9836-5cafe6a2188b",
   "metadata": {},
   "source": [
    "## Ques 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2206a95-e995-4e68-9b36-f1ac1350d224",
   "metadata": {},
   "source": [
    "### Ans: Distance-based anomaly detection methods make the following assumptions:\n",
    "\n",
    "- Anomalies are data points that are located far away from the majority of other data points.\n",
    "- Normal data points are located close to other data points, forming dense clusters.\n",
    "- The distance metric used to measure the similarity between data points is meaningful and appropriate for the data.\n",
    "- The value of the distance metric used to identify anomalies is chosen based on prior knowledge or empirical observations about the data.\n",
    "- The size of the neighborhood around each data point used to estimate the local density of the data is appropriate for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ba544-40c3-4633-b7b4-d83a061d4a20",
   "metadata": {},
   "source": [
    "## Ques 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dba17e3-c2b1-4141-a97f-0eba66bd4328",
   "metadata": {},
   "source": [
    "### Ans: The LOF (Local Outlier Factor) algorithm computes anomaly scores by measuring the local density of a data point relative to its neighbors.\n",
    "### To calculate the anomaly score for a data point, the LOF algorithm first identifies its k nearest neighbors based on a chosen distance metric. It then computes the reachability distance of the data point with respect to each of its k neighbors. The reachability distance is defined as the maximum of the distance between the data point and the k-th nearest neighbor, and the reachability distance of the k-th nearest neighbor.\n",
    "### Next, the LOF algorithm computes the local reachability density (LRD) of the data point, which is defined as the inverse of the average reachability distance of its k neighbors. The LRD measures how densely packed the neighborhood of the data point is relative to the neighborhood of its neighbors.\n",
    "### Finally, the LOF algorithm calculates the local outlier factor (LOF) of the data point as the ratio of the average LRD of its k nearest neighbors to its own LRD. A data point is considered an outlier if its LOF is greater than a specified threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5cf5ac-7dc2-416a-a762-411e976f5e02",
   "metadata": {},
   "source": [
    "## Ques 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98717b-5493-4458-a98e-1073ed5638c9",
   "metadata": {},
   "source": [
    "### Ans: The Isolation Forest algorithm has two main parameters:\n",
    "### n_estimators: This parameter controls the number of trees in the forest. Increasing the number of trees can improve the accuracy of the algorithm but also increases the computation time.\n",
    "### contamination: This parameter determines the expected proportion of anomalies in the dataset. The algorithm uses this parameter to set a threshold for deciding which data points to classify as anomalies. Increasing the value of the contamination parameter increases the proportion of data points classified as anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fa79ee-ba07-45d1-954e-d0766a9fcaab",
   "metadata": {},
   "source": [
    "## Ques 8:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2cbb4-d556-43f6-b780-745860cfe735",
   "metadata": {},
   "source": [
    "### Ans: To compute the anomaly score for a data point using KNN with K=10, we need to compare its k-distance (i.e., the distance to its 10th nearest neighbor) with the average k-distances of its k-nearest neighbors. If the k-distance is significantly larger than the average k-distance, then the data point is considered an anomaly.\n",
    "### In this case, the data point has only 2 neighbors of the same class within a radius of 0.5, which means that it does not have 10 neighbors. Therefore, we cannot compute the k-distance and average k-distance needed for KNN-based anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57042972-253d-4cc7-a1a7-ba81cecddcae",
   "metadata": {},
   "source": [
    "## Ques 9:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce7515-9a8a-4f17-b0e5-b26a6730076f",
   "metadata": {},
   "source": [
    "### Ans: In the Isolation Forest algorithm, the anomaly score for a data point is calculated as the average path length of the data point across all trees in the forest. The intuition behind this is that anomalous data points are expected to have shorter average path lengths than normal data points.\n",
    "### If a data point has an average path length of 5.0 compared to the average path length of the trees, we can calculate its anomaly score as follows:\n",
    "### anomaly score = 2^(-5.0/average path length of trees)\n",
    "### Assuming the average path length of the trees is 10, we can plug in the values to get:\n",
    "### anomaly score = 2^(-5.0/10) = 0.316\n",
    "### Therefore, the anomaly score for the data point is 0.316."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c698a-b657-4499-bad7-98d4c82aaee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
