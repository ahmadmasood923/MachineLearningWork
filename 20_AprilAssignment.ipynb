{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654e9eb4-7ef2-4635-af12-bc184af96ffe",
   "metadata": {},
   "source": [
    "## Ques 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a553a66-8398-473d-b1f3-1233df5a1932",
   "metadata": {},
   "source": [
    "### Ans: The K-Nearest Neighbors (KNN) algorithm is a simple yet powerful machine learning algorithm used for classification and regression tasks. Given a new input, it finds the K closest data points in the training dataset and predicts the label of the new input based on the majority label of those K neighbors. The value of K is a hyperparameter that can be chosen by the user. KNN is a non-parametric algorithm, meaning it does not make any assumptions about the underlying data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800dc4f1-893c-4e69-bf30-8d27219abe86",
   "metadata": {},
   "source": [
    "## Ques 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f3a41-bab5-4f49-a33f-fdf2d798a90c",
   "metadata": {},
   "source": [
    "### Ans: The value of K in K-Nearest Neighbors (KNN) is a hyperparameter that controls the number of neighboring points used to classify a new data point. Choosing the appropriate value of K is important to achieve optimal performance in KNN.\n",
    "### There are several methods to choose the value of K in KNN, including:\n",
    "### Trial and error: Start by trying out different values of K and evaluate the performance of the model using a validation set or cross-validation. Choose the value of K that gives the best performance.\n",
    "### Rule of thumb: A common rule of thumb is to set K to the square root of the number of data points in the training set. However, this rule may not always be optimal and should be used as a starting point for tuning K.\n",
    "### Grid search: Perform a grid search over a range of K values and choose the value of K that gives the best performance. This method can be computationally expensive but is useful for exploring a large range of hyperparameters.\n",
    "### Bayesian optimization: Use Bayesian optimization to efficiently search for the optimal value of K. This method can be more efficient than grid search for high-dimensional hyperparameter spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35b559e-8447-44f2-b016-e0384ba193b6",
   "metadata": {},
   "source": [
    "## Ques 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d859ee-685f-48c3-9237-6e78f3513de1",
   "metadata": {},
   "source": [
    "### Ans: K-Nearest Neighbor (KNN) is a type of supervised learning algorithm that can be used for both classification and regression tasks. The main difference between KNN classifier and KNN regressor lies in the type of output they produce.\n",
    "### In KNN classification, the output is a categorical variable, which means the algorithm is used to classify data points into one of several predefined classes. The KNN classifier determines the class of a new data point based on the classes of its k-nearest neighbors in the training data. The output of the KNN classifier is the class label assigned to the new data point.\n",
    "### On the other hand, in KNN regression, the output is a continuous variable, which means the algorithm is used to predict a numeric value. The KNN regressor determines the value of a new data point based on the average (or median) of the values of its k-nearest neighbors in the training data. The output of the KNN regressor is the predicted value for the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e358185b-80a8-4b49-b8be-4a3df2bf26db",
   "metadata": {},
   "source": [
    "## Ques 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e8935-927d-4822-9e03-ae553beabeef",
   "metadata": {},
   "source": [
    "### Ans: The performance of KNN can be measured using various evaluation metrics depending on whether you are using KNN for classification or regression tasks. Here are some commonly used evaluation metrics for KNN:\n",
    "### For KNN classification:\n",
    "### Accuracy: The most common evaluation metric for classification tasks is accuracy, which measures the proportion of correctly classified instances out of all instances in the dataset.\n",
    "### Precision and recall: Precision measures the proportion of true positives (correctly classified positive instances) out of all predicted positives, while recall measures the proportion of true positives out of all actual positives.\n",
    "### F1 score: The F1 score is the harmonic mean of precision and recall and is a more balanced measure than accuracy when the classes are imbalanced.\n",
    "### Confusion matrix: A confusion matrix is a table that summarizes the classification results and shows the number of true positives, false positives, true negatives, and false negatives.\n",
    "### For KNN regression:\n",
    "### Mean squared error (MSE): MSE measures the average squared difference between the predicted and actual values, and is a common metric for regression tasks.\n",
    "### Root mean squared error (RMSE): RMSE is the square root of MSE and is more interpretable as it is in the same units as the target variable.\n",
    "### R-squared (R2): R2 measures the proportion of variance in the target variable that is explained by the model and is a value between 0 and 1. A higher R2 indicates a better fit of the model to the data.\n",
    "### Mean absolute error (MAE): MAE measures the average absolute difference between the predicted and actual values and is less sensitive to outliers than MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ea232-5c68-4590-98a4-e422bb836388",
   "metadata": {},
   "source": [
    "## Ques 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805985e2-2f67-46cf-a088-0c5ead93d763",
   "metadata": {},
   "source": [
    "### Ans: The curse of dimensionality is a phenomenon that occurs in machine learning algorithms, including the k-nearest neighbor (KNN) algorithm, when the number of features or dimensions in the dataset becomes too high relative to the number of data points. In this case, the data becomes increasingly sparse in the high-dimensional space, making it difficult for KNN to identify nearest neighbors accurately.\n",
    "### As the number of dimensions increases, the number of data points required to fill the space becomes exponentially large. This can lead to overfitting or underfitting of the model, and can also result in increased computation time and memory requirements. In addition, the distance metrics used by KNN may become less meaningful as the dimensions increase, further reducing the accuracy of the algorithm.\n",
    "### To mitigate the curse of dimensionality in KNN, it is often recommended to use feature selection or dimensionality reduction techniques to reduce the number of dimensions in the dataset. Another approach is to use distance-based clustering algorithms, such as k-means, to identify clusters of similar data points before applying KNN to each cluster separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d8257-adbd-419f-863c-654ff5acaf81",
   "metadata": {},
   "source": [
    "## Ques 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67b249-c754-4dfc-a912-0d4512a0c913",
   "metadata": {},
   "source": [
    "### Ans: There are different methods to handle missing values in K-Nearest Neighbors (KNN) algorithm. Here are some common strategies:\n",
    "### Mean/Median Imputation: Replace the missing values with the mean or median value of the feature across all the available data points. This method assumes that the missing values are missing at random (MAR).\n",
    "### Mode Imputation: Replace the missing values with the mode value of the feature across all the available data points. This method is suitable for categorical features.\n",
    "### KNN Imputation: Use the KNN algorithm itself to impute the missing values. For each missing value, find the k-nearest neighbors that have non-missing values for that feature and take the average of their values. This method can be computationally expensive, but it can also lead to better imputation accuracy.\n",
    "### Deletion: Remove the data points that have missing values. This method can result in a loss of information and can be biased if the missing values are not missing completely at random (MCAR).\n",
    "### The choice of method depends on the nature of the data and the amount of missing data. It's important to consider the potential impact of each method on the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c2776b-05fe-4ff5-b499-99c8ff41ce2c",
   "metadata": {},
   "source": [
    "## Ques 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6da36-670f-40fc-b836-6a28b6b9678b",
   "metadata": {},
   "source": [
    "### Ans: The KNN (K-Nearest Neighbors) algorithm can be used for both classification and regression problems. However, the performance of the KNN classifier and regressor can differ based on the type of problem and the data.\n",
    "### Here are some differences between the KNN classifier and regressor:\n",
    "### Output: The KNN classifier predicts the class label of the test data based on the class labels of the K-nearest neighbors. The KNN regressor predicts the output value of the test data based on the average of the output values of the K-nearest neighbors.\n",
    "### Metric: The KNN classifier typically uses a distance metric such as Euclidean distance to calculate the distance between the test data and the training data. The KNN regressor also uses a distance metric but the distance is calculated between the output values of the training data and the test data.\n",
    "### Decision boundary: The decision boundary of the KNN classifier is non-linear and can be complex. The decision boundary of the KNN regressor is a smooth curve.\n",
    "### Performance: The performance of the KNN classifier can be affected by the curse of dimensionality when the number of features is large. The KNN regressor can also be affected by the curse of dimensionality but to a lesser extent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4fdbf-3353-4227-a458-9d9aaa896db6",
   "metadata": {},
   "source": [
    "## Ques 8:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda2952-250d-49cb-b7d7-b13ac490791e",
   "metadata": {},
   "source": [
    "### Ans: Strengths of KNN algorithm:\n",
    "### Simple and easy to implement.\n",
    "### Non-parametric nature, meaning it does not make any assumptions about the underlying distribution of data.\n",
    "### Works well with both linear and non-linear data.\n",
    "### Can handle multi-class classification problems.\n",
    "### Can be used for both classification and regression tasks.\n",
    "### Weaknesses of KNN algorithm:\n",
    "### Computationally expensive, especially with large datasets.\n",
    "### Sensitive to the choice of distance metric and value of k.\n",
    "### Requires a lot of memory to store the training data.\n",
    "### Cannot handle missing data, and can be affected by irrelevant or noisy features.\n",
    "### Here are some ways to address the weaknesses of the KNN algorithm:\n",
    "### Use dimensionality reduction techniques like Principal Component Analysis (PCA) to reduce the number of features and improve computation efficiency.\n",
    "### Use cross-validation to find the optimal value of k and distance metric for a given dataset.\n",
    "### Use ensemble methods like Bagging or Boosting to reduce the variance and improve the accuracy of KNN.\n",
    "### Use algorithms that can handle missing data, like imputation techniques or other machine learning algorithms.\n",
    "### Use feature selection techniques to remove irrelevant or noisy features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a9f27-5f02-4d09-8549-dedc851fadd2",
   "metadata": {},
   "source": [
    "## Ques 9:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a84566-ea03-45bd-9f40-011edcfd2e0a",
   "metadata": {},
   "source": [
    "### Ans: Euclidean distance is the straight-line distance between two points in Euclidean space. It is the most commonly used distance metric in KNN algorithm. It is calculated as the square root of the sum of the squared differences between each dimension of the two data points. \n",
    "### Manhattan distance, also known as Taxicab distance or L1 distance, is the sum of the absolute differences between each dimension of the two data points. It is called Manhattan distance because it measures the distance between two points in a city where one can only travel along the street grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863be17-4a03-4261-89e1-800e18e55097",
   "metadata": {},
   "source": [
    "## Ques 10:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82481a12-9f3c-4d01-8baf-b95c51d39df4",
   "metadata": {},
   "source": [
    "### Ans: Feature scaling is an important preprocessing step in KNN algorithm that involves scaling or normalizing the features of the input data. The goal of feature scaling is to bring all the features to a common scale so that each feature contributes equally to the distance metric used by the KNN algorithm.\n",
    "### KNN algorithm uses a distance metric (such as Euclidean distance or Manhattan distance) to measure the similarity between data points. The distance metric is calculated based on the values of the features of the data points. If the features are on different scales, then the feature with the larger scale will dominate the distance metric. This means that the KNN algorithm will give more weight to the features with larger scales, and the features with smaller scales will have less impact on the final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070d9a3-de28-47fb-aae4-5c085ca84c97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
