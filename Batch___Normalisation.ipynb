{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ques 1:"
      ],
      "metadata": {
        "id": "YisQvKmtplvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ques 1:"
      ],
      "metadata": {
        "id": "SK46qMTBofIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ans: Batch normalization is a technique used in artificial neural networks to improve the training speed and stability of deep learning models. It normalizes the inputs of a layer by adjusting and scaling them using the mean and variance computed over a mini-batch of training samples.\n",
        "### In a typical neural network, the distribution of input values changes during training as the model's parameters get updated. This phenomenon is known as the internal covariate shift. It makes the learning process slower and requires careful initialization and tuning of network parameters."
      ],
      "metadata": {
        "id": "_5YEdR_iogTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ques 2:"
      ],
      "metadata": {
        "id": "-fNOmyXPog_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ans: Faster convergence: By normalizing the inputs and reducing the internal covariate shift, batch normalization helps accelerate the training process. It allows the model to converge faster and achieve better accuracy in fewer training iterations.\n",
        "### Improved gradient flow: Normalizing the inputs reduces the impact of vanishing or exploding gradients. This stabilization of gradients helps in training deeper networks by improving the flow of gradients through the layers.\n",
        "### Regularization effect: Batch normalization acts as a regularizer by adding a small amount of noise to the inputs. This noise can help prevent overfitting and improve the model's generalization ability.\n",
        "### Reduced sensitivity to initialization: Batch normalization makes the network less sensitive to the choice of initial weights. It reduces the need for careful weight initialization techniques, making the training process more robust."
      ],
      "metadata": {
        "id": "xIhepktiohRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ques 3:"
      ],
      "metadata": {
        "id": "JdIIBRBxpBSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ans: Normalization Step:\n",
        "### The normalization step in batch normalization involves adjusting and scaling the inputs of a layer using the mean and variance computed over a mini-batch of training samples. Here's a breakdown of the normalization process:\n",
        "### a. Mini-Batch Statistics: Given a mini-batch of input data for a specific layer, the mean (μ) and variance (σ^2) of the batch are computed. These statistics are calculated independently for each feature or channel of the input data.\n",
        "### b. Mean and Variance Calculation: The mean is calculated by taking the average of the values for each feature in the mini-batch. The variance is computed by taking the average of the squared differences between each value and the mean.\n",
        "### c. Normalization: The inputs in the mini-batch are then normalized by subtracting the mean and dividing by the square root of the variance. This step transforms the inputs to have zero mean and unit variance, resulting in a normalized distribution.\n",
        "### Learnable Parameters:\n",
        "### Batch normalization also introduces learnable parameters to the normalized inputs. These parameters allow the network to adjust the normalized values based on the specific needs of the layer. The learnable parameters include:\n",
        "### a. Scale (γ): The scale parameter is a learnable parameter that allows the network to amplify or diminish the normalized values. It provides the flexibility to the layer to rescale the outputs according to its requirements.\n",
        "### b. Shift (β): The shift parameter is another learnable parameter that enables the network to add an offset or bias to the normalized values. It helps the layer to adjust the mean of the outputs based on its specific requirements."
      ],
      "metadata": {
        "id": "LGR0LkixpBnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observing results before and after applying Batch Normalization"
      ],
      "metadata": {
        "id": "xHG_iwt1xOzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before applying Batch Normalization"
      ],
      "metadata": {
        "id": "gJxSI8XzxZlY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IryMolUxJJb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset of fashion MNIST\n",
        "(X_train_full,y_train_full), (X_test,y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "X_train_full = X_train_full/255.0\n",
        "X_test = X_test/255.0\n",
        "X_valid, X_train = X_train_full[:5000],X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000],y_train_full[5000:]"
      ],
      "metadata": {
        "id": "4mAM7cSIx1QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating layer of model\n",
        "tf.random.set_seed(42) # For getting similar output (optional)\n",
        "np.random.seed(42)     # For getting similar output (optional)\n",
        "LAYERS = [\n",
        "    tf.keras.layers.Flatten(input_shape=[28,28]),\n",
        "    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.LeakyReLU(),\n",
        "    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.LeakyReLU(),\n",
        "    tf.keras.layers.Dense(10,activation='softmax')\n",
        "]\n",
        "\n",
        "model = tf.keras.models.Sequential(LAYERS)"
      ],
      "metadata": {
        "id": "Bhs1oyVhyKkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubaTPQwn0OD8",
        "outputId": "2551d01b-3e80-4727-fdec-e5d9917a14b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kE3XtVD045y",
        "outputId": "e2a293db-c397-4dd1-a818-6437d12a2998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_3 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 300)               235500    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 300)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 100)               30100     \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 100)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now training and calculating the training time\n",
        "\n",
        "# starting time\n",
        "start = time.time()\n",
        "\n",
        "history = model.fit(X_train,y_train,epochs=10,\n",
        "                    validation_data=[X_valid,y_valid], verbose=2)\n",
        "\n",
        "# ending time\n",
        "end = time.time()\n",
        "\n",
        "# Total time taken\n",
        "print(f\"Runtime of the program is {end - start}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeNvxpsj1Anm",
        "outputId": "0f904f97-6ccf-42b3-b203-d7d942c90cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 - 10s - loss: 0.6904 - accuracy: 0.7692 - val_loss: 0.5013 - val_accuracy: 0.8304 - 10s/epoch - 6ms/step\n",
            "Epoch 2/10\n",
            "1719/1719 - 9s - loss: 0.4829 - accuracy: 0.8314 - val_loss: 0.4335 - val_accuracy: 0.8530 - 9s/epoch - 5ms/step\n",
            "Epoch 3/10\n",
            "1719/1719 - 8s - loss: 0.4414 - accuracy: 0.8444 - val_loss: 0.5291 - val_accuracy: 0.8022 - 8s/epoch - 5ms/step\n",
            "Epoch 4/10\n",
            "1719/1719 - 9s - loss: 0.4177 - accuracy: 0.8540 - val_loss: 0.4014 - val_accuracy: 0.8662 - 9s/epoch - 5ms/step\n",
            "Epoch 5/10\n",
            "1719/1719 - 9s - loss: 0.4023 - accuracy: 0.8590 - val_loss: 0.3872 - val_accuracy: 0.8652 - 9s/epoch - 5ms/step\n",
            "Epoch 6/10\n",
            "1719/1719 - 8s - loss: 0.3862 - accuracy: 0.8648 - val_loss: 0.3791 - val_accuracy: 0.8716 - 8s/epoch - 5ms/step\n",
            "Epoch 7/10\n",
            "1719/1719 - 8s - loss: 0.3754 - accuracy: 0.8678 - val_loss: 0.3742 - val_accuracy: 0.8694 - 8s/epoch - 5ms/step\n",
            "Epoch 8/10\n",
            "1719/1719 - 8s - loss: 0.3657 - accuracy: 0.8704 - val_loss: 0.3912 - val_accuracy: 0.8604 - 8s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "1719/1719 - 9s - loss: 0.3564 - accuracy: 0.8742 - val_loss: 0.3673 - val_accuracy: 0.8684 - 9s/epoch - 5ms/step\n",
            "Epoch 10/10\n",
            "1719/1719 - 9s - loss: 0.3486 - accuracy: 0.8767 - val_loss: 0.3615 - val_accuracy: 0.8710 - 9s/epoch - 5ms/step\n",
            "Runtime of the program is 142.88154768943787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion:\n",
        "- Runtime of the program is 142.88s\n",
        "- Accuracy of the model is 0.8710\n"
      ],
      "metadata": {
        "id": "e9tVw7CM3F4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After applying Batch Normalization"
      ],
      "metadata": {
        "id": "Y7Vk9jf53jmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the previous model\n",
        "del model"
      ],
      "metadata": {
        "id": "Z7aRJEK51yJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model with batch normalization\n",
        "LAYERS_BN = [\n",
        "    tf.keras.layers.Flatten(input_shape=[28,28]),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(300, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(10,activation='softmax')\n",
        "]\n",
        "model = tf.keras.models.Sequential(LAYERS_BN)"
      ],
      "metadata": {
        "id": "4RbS1bsY3sry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj-DognQ4eDg",
        "outputId": "f97b5890-511d-4266-b1cd-c297e7f0bce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_6 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 784)              3136      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 300)               235500    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 300)              1200      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 100)               30100     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 100)              400       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bn1 = model.layers[1]"
      ],
      "metadata": {
        "id": "jkxhuzlV4r1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for variable in bn1.variables:\n",
        "  print(variable.name, variable.trainable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_gHOJz55RW_",
        "outputId": "1334f1fd-62f4-4232-a20b-8ce421e04c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_normalization/gamma:0 True\n",
            "batch_normalization/beta:0 True\n",
            "batch_normalization/moving_mean:0 False\n",
            "batch_normalization/moving_variance:0 False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9Kd2qFX5g47",
        "outputId": "28713859-88b5-45fc-df1b-90f45d7bb94d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now training and calculating the training time\n",
        "\n",
        "# starting time\n",
        "start = time.time()\n",
        "\n",
        "history = model.fit(X_train,y_train,epochs=10,\n",
        "                    validation_data=[X_valid,y_valid], verbose=2)\n",
        "\n",
        "# ending time\n",
        "end = time.time()\n",
        "\n",
        "# Total time taken\n",
        "print(f\"Runtime of the program is {end - start}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9ijkieR5p24",
        "outputId": "1fbc1ad0-c7bb-458b-9c24-3b199b2aa6dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 - 13s - loss: 0.2264 - accuracy: 0.9178 - val_loss: 0.3086 - val_accuracy: 0.8934 - 13s/epoch - 8ms/step\n",
            "Epoch 2/10\n",
            "1719/1719 - 13s - loss: 0.2176 - accuracy: 0.9209 - val_loss: 0.3108 - val_accuracy: 0.8912 - 13s/epoch - 7ms/step\n",
            "Epoch 3/10\n",
            "1719/1719 - 13s - loss: 0.2103 - accuracy: 0.9235 - val_loss: 0.3117 - val_accuracy: 0.8868 - 13s/epoch - 8ms/step\n",
            "Epoch 4/10\n",
            "1719/1719 - 13s - loss: 0.1995 - accuracy: 0.9273 - val_loss: 0.3166 - val_accuracy: 0.8908 - 13s/epoch - 8ms/step\n",
            "Epoch 5/10\n",
            "1719/1719 - 13s - loss: 0.1945 - accuracy: 0.9286 - val_loss: 0.2987 - val_accuracy: 0.8986 - 13s/epoch - 7ms/step\n",
            "Epoch 6/10\n",
            "1719/1719 - 13s - loss: 0.1837 - accuracy: 0.9338 - val_loss: 0.3120 - val_accuracy: 0.8914 - 13s/epoch - 8ms/step\n",
            "Epoch 7/10\n",
            "1719/1719 - 14s - loss: 0.1787 - accuracy: 0.9356 - val_loss: 0.3153 - val_accuracy: 0.8964 - 14s/epoch - 8ms/step\n",
            "Epoch 8/10\n",
            "1719/1719 - 13s - loss: 0.1720 - accuracy: 0.9375 - val_loss: 0.3216 - val_accuracy: 0.8924 - 13s/epoch - 8ms/step\n",
            "Epoch 9/10\n",
            "1719/1719 - 16s - loss: 0.1655 - accuracy: 0.9407 - val_loss: 0.3192 - val_accuracy: 0.8954 - 16s/epoch - 9ms/step\n",
            "Epoch 10/10\n",
            "1719/1719 - 17s - loss: 0.1581 - accuracy: 0.9430 - val_loss: 0.3210 - val_accuracy: 0.9014 - 17s/epoch - 10ms/step\n",
            "Runtime of the program is 142.1921534538269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "- Runtime of the program is 142.19s\n",
        "- Accuracy of the model is 0.9430"
      ],
      "metadata": {
        "id": "6621vchq61KV"
      }
    }
  ]
}