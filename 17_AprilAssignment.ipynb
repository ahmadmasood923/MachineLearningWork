{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d8f995-5716-4f09-9321-6033b0e5816c",
   "metadata": {},
   "source": [
    "## Ques 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14a206-96e9-4b6a-83e6-6f8a51296225",
   "metadata": {},
   "source": [
    "### Ans: Gradient Boosting Regression is a machine learning algorithm used for regression problems, which involves predicting a continuous output value. It is a boosting algorithm that combines multiple weak regression models to create a strong regression model. The algorithm works by iteratively adding weak models to the ensemble, each model correcting the errors of the previous ones.\n",
    "### The key idea behind Gradient Boosting Regression is to fit each new model to the residual errors of the previous models, which are the differences between the predicted and the actual output values. In each iteration, the algorithm trains a new weak regression model to predict the residual errors of the previous models, and adds it to the ensemble. The final prediction of the ensemble is obtained by adding up the predictions of all the weak models, weighted by their learning rate, which controls the contribution of each model to the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0c81d-d4bd-4c7c-9f76-37f3fa2fd183",
   "metadata": {},
   "source": [
    "## Ques 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d882e-df17-43c8-a498-772066d7fd5c",
   "metadata": {},
   "source": [
    "### Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "708a5de3-74aa-42f0-acbe-30ec54ed82e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1017.9179834907184\n",
      "R-squared: 0.9074439095441444\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Generate a small regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "split = int(len(X)*0.8)\n",
    "X_train, X_test, y_train, y_test = X[:split], X[split:], y[:split], y[split:]\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the predictions to the mean of the target variable\n",
    "        self.mean = np.mean(y)\n",
    "        self.predictions = np.full(len(y), self.mean)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # Calculate the residual errors\n",
    "            residuals = y - self.predictions\n",
    "\n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Add the tree to the ensemble\n",
    "            self.models.append(tree)\n",
    "            self.weights.append(self.learning_rate)\n",
    "\n",
    "            # Update the predictions by adding the weighted predictions of the new tree\n",
    "            self.predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by adding up the weighted predictions of all trees\n",
    "        y_pred = np.full(len(X), self.mean)\n",
    "        for i, tree in enumerate(self.models):\n",
    "            y_pred += self.weights[i] * tree.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "# Train a gradient boosting regression model\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b72ef68-65ab-4fd4-8689-2fade20c2d16",
   "metadata": {},
   "source": [
    "## Ques 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b5fec-fbb2-4d12-94a2-860e87b60676",
   "metadata": {},
   "source": [
    "### Ans: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea379d-2641-400b-aa59-3f71cbec115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=GradientBoostingRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Mean Squared Error:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b50c2b-314e-49ac-96d4-3b7539556f9f",
   "metadata": {},
   "source": [
    "## Ques 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b8c456-03b1-445c-addb-0052d553c337",
   "metadata": {},
   "source": [
    "### Ans: In Gradient Boosting, a weak learner is a model that performs slightly better than random guessing. Typically, decision trees with a small number of nodes are used as weak learners in Gradient Boosting. The idea is to train an ensemble of weak learners sequentially, where each weak learner is trained to improve the errors made by the previous weak learners in the ensemble.\n",
    "### Each weak learner is fit on the residuals of the previous predictions, so it focuses on the examples that were poorly predicted by the previous weak learners. By iteratively adding new weak learners to the ensemble, the model gradually learns to make better predictions on the training data.\n",
    "### The strength of Gradient Boosting comes from the ability to combine many weak learners into a powerful ensemble model that can capture complex relationships between the features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeee50e-f158-4c40-b2c2-d62162b18a84",
   "metadata": {},
   "source": [
    "## Ques 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5bd5aa-b24a-4441-897d-bc2befc82b8c",
   "metadata": {},
   "source": [
    "### Ans: The intuition behind the Gradient Boosting algorithm is to iteratively add new weak learners to an ensemble in a way that focuses on the examples that were poorly predicted by the previous learners.\n",
    "### At each iteration, the algorithm trains a new weak learner to fit the residuals (i.e., the differences between the predicted values and the actual values) of the previous weak learners. By adding the new weak learner to the ensemble, the algorithm improves the model's ability to predict the training data.\n",
    "### The name \"Gradient Boosting\" comes from the fact that the algorithm uses gradient descent to optimize the objective function (i.e., the loss function) of the model. Specifically, the algorithm calculates the negative gradient of the loss function with respect to the predicted values, and then fits a weak learner to these negative gradients.\n",
    "### By iteratively adding new weak learners and adjusting the predictions to minimize the negative gradient of the loss function, the algorithm gradually improves the model's predictions on the training data. The final model is an ensemble of all the weak learners, weighted by their learning rate, which determines the contribution of each weak learner to the final prediction.\n",
    "### Overall, the intuition behind Gradient Boosting is to build an ensemble of weak learners that can capture complex relationships between the features and the target variable by focusing on the examples that are difficult to predict with the previous learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d976c160-930d-4717-a252-946ef389314a",
   "metadata": {},
   "source": [
    "## Ques 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42c16e-f4ec-4577-a310-df7133b3ed60",
   "metadata": {},
   "source": [
    "### Ans: Gradient Boosting builds an ensemble of weak learners sequentially, where each weak learner is trained to improve the errors made by the previous weak learners in the ensemble. The algorithm works as follows:\n",
    "### Initialize the ensemble with a simple model, such as a decision tree with only one node.\n",
    "### For each iteration:\n",
    "### a. Calculate the residuals of the previous predictions by subtracting the predicted values from the actual values.\n",
    "### b. Train a new weak learner on the residuals using a subset of the training data.\n",
    "### c. Calculate the contribution of the new weak learner to the ensemble by multiplying its predictions by a learning rate (a hyperparameter between 0 and 1 that determines the contribution of each weak learner).\n",
    "### d. Add the contribution of the new weak learner to the ensemble.\n",
    "### Repeat step 2 for a fixed number of iterations or until a convergence criterion is met.\n",
    "### Each weak learner is trained on a subset of the training data, using a technique called \"boosting\". Boosting samples the training data randomly with replacement (a process called \"bagging\"), but it assigns higher weights to the examples that were poorly predicted by the previous weak learners. This ensures that the new weak learner focuses on the examples that are difficult to predict with the previous learners.\n",
    "### The final model is an ensemble of all the weak learners, weighted by their learning rate. The intuition behind this ensemble is that each weak learner contributes a small amount of information to the final prediction, but together they can capture complex relationships between the features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f41f62f-7d09-462a-8030-2f9fd22c59db",
   "metadata": {},
   "source": [
    "## Ques 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfd236-1b23-4562-86f1-2dd3f1d22758",
   "metadata": {},
   "source": [
    "### Ans: The mathematical intuition behind Gradient Boosting can be summarized in the following steps:\n",
    "\n",
    "- Define an initial model that makes a prediction based on the mean of the target variable.\n",
    "- Calculate the negative gradient of the loss function with respect to the initial model's predictions. This gives the \"pseudo-residuals\", which are the negative gradients of the loss function.\n",
    "- Train a new weak learner to fit the pseudo-residuals, using the features as input and the pseudo-residuals as the target variable.\n",
    "- Add the new weak learner's predictions to the initial model's predictions, weighted by a learning rate, to get the updated predictions.\n",
    "- Repeat steps 2-4 for a fixed number of iterations, or until a convergence criterion is met.\n",
    "- The final model is an ensemble of all the weak learners, weighted by their learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e685d6-6512-4ada-bc56-2f0297d5ab48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
